{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the tutorial:\n",
    "\n",
    "- https://thinkingneuron.com/how-to-classify-text-using-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the support ticket data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19796, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>urgency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connection issues with assigned address hi fac...</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cannot access hi cannot access fallowing link ...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>re address shown valid dear colleagues remarke...</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sent tuesday critical alert following alert oc...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>code spelling mistake hello should discover fo...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>annual leave hello sent last week about previo...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>report working hello dear last two weeks have ...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>more access lost access please reset password ...</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>open credentials required please assist instal...</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dear please ask our supplier for price quotati...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body urgency\n",
       "0  connection issues with assigned address hi fac...      P1\n",
       "1  cannot access hi cannot access fallowing link ...      P2\n",
       "2  re address shown valid dear colleagues remarke...      P1\n",
       "3  sent tuesday critical alert following alert oc...      P2\n",
       "4  code spelling mistake hello should discover fo...      P2\n",
       "5  annual leave hello sent last week about previo...      P2\n",
       "6  report working hello dear last two weeks have ...      P2\n",
       "7  more access lost access please reset password ...      P1\n",
       "8  open credentials required please assist instal...      P1\n",
       "9  dear please ask our supplier for price quotati...      P2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "# Reading the data\n",
    "TicketData=pd.read_csv('./data/supportTicketData.csv')\n",
    " \n",
    "# Printing number of rows and columns\n",
    "print(TicketData.shape)\n",
    " \n",
    "# Printing sample rows\n",
    "TicketData.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the distribution of the Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urgency\n",
      "P1    6748\n",
      "P2    5528\n",
      "P3    7520\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of unique values for urgency column\n",
    "# You can see there are 3 ticket types\n",
    "print(TicketData.groupby('urgency').size())\n",
    " \n",
    "# Plotting the bar chart\n",
    "%matplotlib inline\n",
    "TicketData.groupby('urgency').size().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization: converting text data to numeric\n",
    "This step will help to remove all the stopwords and create a document term matrix.\n",
    "\n",
    "We will use this matrix to do further processing. For each word in the document term matrix, we will use the Word2Vec numeric vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19796, 9100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>abeam</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abnormally</th>\n",
       "      <th>...</th>\n",
       "      <th>zig</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipped</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zipping</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooming</th>\n",
       "      <th>Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ab  abandon  abandoned  abc  abeam  abilities  ability  able  abnormal  \\\n",
       "0   0        0          0    0      0          0        0     0         0   \n",
       "1   0        0          0    0      0          0        0     0         0   \n",
       "2   0        0          0    0      0          0        0     0         0   \n",
       "3   0        0          0    0      0          0        0     0         0   \n",
       "4   0        0          0    0      0          0        0     0         0   \n",
       "\n",
       "   abnormally  ...  zig  zip  zipped  zipper  zipping  zone  zones  zoom  \\\n",
       "0           0  ...    0    0       0       0        0     0      0     0   \n",
       "1           0  ...    0    0       0       0        0     0      0     0   \n",
       "2           0  ...    0    0       0       0        0     0      0     0   \n",
       "3           0  ...    0    0       0       0        0     0      0     0   \n",
       "4           0  ...    0    0       0       0        0     0      0     0   \n",
       "\n",
       "   zooming  Priority  \n",
       "0        0        P1  \n",
       "1        0        P2  \n",
       "2        0        P1  \n",
       "3        0        P2  \n",
       "4        0        P2  \n",
       "\n",
       "[5 rows x 9100 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorization of text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ticket Data\n",
    "corpus = TicketData['body'].values\n",
    "\n",
    "# Creating the vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Converting the text to numeric data\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "# Preparing Data frame For machine learning\n",
    "# Priority column acts as a target variable and other columns as predictors\n",
    "CountVectorizedData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "CountVectorizedData['Priority']=TicketData['urgency']\n",
    "print(CountVectorizedData.shape)\n",
    "CountVectorizedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec conversion:\n",
    "Now we will use the Word2Vec representation of words to convert the above document term matrix to a smaller matrix, where the columns are the sum of the vectors for each word present in the document.\n",
    "\n",
    "For example, look at the below diagram. The flow is shown for one sentence, the same happens for every sentence in the corpus.\n",
    "\n",
    "The numeric representation of each word is taken from Word2Vec.\n",
    "All the vectors are added, hence producing a single vector\n",
    "That single vector represents the information of the sentence, hence treated as one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#Loading the word vectors from Google trained word2Vec model\n",
    "GoogleModel = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each word is a vector of 300 numbers\n",
    "GoogleModel['hello'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05419922,  0.01708984, -0.00527954,  0.33203125, -0.25      ,\n",
       "       -0.01397705, -0.15039062, -0.265625  ,  0.01647949,  0.3828125 ,\n",
       "       -0.03295898, -0.09716797, -0.16308594, -0.04443359,  0.00946045,\n",
       "        0.18457031,  0.03637695,  0.16601562,  0.36328125, -0.25585938,\n",
       "        0.375     ,  0.171875  ,  0.21386719, -0.19921875,  0.13085938,\n",
       "       -0.07275391, -0.02819824,  0.11621094,  0.15332031,  0.09082031,\n",
       "        0.06787109, -0.0300293 , -0.16894531, -0.20800781, -0.03710938,\n",
       "       -0.22753906,  0.26367188,  0.012146  ,  0.18359375,  0.31054688,\n",
       "       -0.10791016, -0.19140625,  0.21582031,  0.13183594, -0.03515625,\n",
       "        0.18554688, -0.30859375,  0.04785156, -0.10986328,  0.14355469,\n",
       "       -0.43554688, -0.0378418 ,  0.10839844,  0.140625  , -0.10595703,\n",
       "        0.26171875, -0.17089844,  0.39453125,  0.12597656, -0.27734375,\n",
       "       -0.28125   ,  0.14746094, -0.20996094,  0.02355957,  0.18457031,\n",
       "        0.00445557, -0.27929688, -0.03637695, -0.29296875,  0.19628906,\n",
       "        0.20703125,  0.2890625 , -0.20507812,  0.06787109, -0.43164062,\n",
       "       -0.10986328, -0.2578125 , -0.02331543,  0.11328125,  0.23144531,\n",
       "       -0.04418945,  0.10839844, -0.2890625 , -0.09521484, -0.10351562,\n",
       "       -0.0324707 ,  0.07763672, -0.13378906,  0.22949219,  0.06298828,\n",
       "        0.08349609,  0.02929688, -0.11474609,  0.00534058, -0.12988281,\n",
       "        0.02514648,  0.08789062,  0.24511719, -0.11474609, -0.296875  ,\n",
       "       -0.59375   , -0.29492188, -0.13378906,  0.27734375, -0.04174805,\n",
       "        0.11621094,  0.28320312,  0.00241089,  0.13867188, -0.00683594,\n",
       "       -0.30078125,  0.16210938,  0.01171875, -0.13867188,  0.48828125,\n",
       "        0.02880859,  0.02416992,  0.04736328,  0.05859375, -0.23828125,\n",
       "        0.02758789,  0.05981445, -0.03857422,  0.06933594,  0.14941406,\n",
       "       -0.10888672, -0.07324219,  0.08789062,  0.27148438,  0.06591797,\n",
       "       -0.37890625, -0.26171875, -0.13183594,  0.09570312, -0.3125    ,\n",
       "        0.10205078,  0.03063965,  0.23632812,  0.00582886,  0.27734375,\n",
       "        0.20507812, -0.17871094, -0.31445312, -0.01586914,  0.13964844,\n",
       "        0.13574219,  0.0390625 , -0.29296875,  0.234375  , -0.33984375,\n",
       "       -0.11816406,  0.10644531, -0.18457031, -0.02099609,  0.02563477,\n",
       "        0.25390625,  0.07275391,  0.13574219, -0.00138092, -0.2578125 ,\n",
       "       -0.2890625 ,  0.10107422,  0.19238281, -0.04882812,  0.27929688,\n",
       "       -0.3359375 , -0.07373047,  0.01879883, -0.10986328, -0.04614258,\n",
       "        0.15722656,  0.06689453, -0.03417969,  0.16308594,  0.08642578,\n",
       "        0.44726562,  0.02026367, -0.01977539,  0.07958984,  0.17773438,\n",
       "       -0.04370117, -0.00952148,  0.16503906,  0.17285156,  0.23144531,\n",
       "       -0.04272461,  0.02355957,  0.18359375, -0.41601562, -0.01745605,\n",
       "        0.16796875,  0.04736328,  0.14257812,  0.08496094,  0.33984375,\n",
       "        0.1484375 , -0.34375   , -0.14160156, -0.06835938, -0.14648438,\n",
       "       -0.02844238,  0.07421875, -0.07666016,  0.12695312,  0.05859375,\n",
       "       -0.07568359, -0.03344727,  0.23632812, -0.16308594,  0.16503906,\n",
       "        0.1484375 , -0.2421875 , -0.3515625 , -0.30664062,  0.00491333,\n",
       "        0.17675781,  0.46289062,  0.14257812, -0.25      , -0.25976562,\n",
       "        0.04370117,  0.34960938,  0.05957031,  0.07617188, -0.02868652,\n",
       "       -0.09667969, -0.01281738,  0.05859375, -0.22949219, -0.1953125 ,\n",
       "       -0.12207031,  0.20117188, -0.42382812,  0.06005859,  0.50390625,\n",
       "        0.20898438,  0.11230469, -0.06054688,  0.33203125,  0.07421875,\n",
       "       -0.05786133,  0.11083984, -0.06494141,  0.05639648,  0.01757812,\n",
       "        0.08398438,  0.13769531,  0.2578125 ,  0.16796875, -0.16894531,\n",
       "        0.01794434,  0.16015625,  0.26171875,  0.31640625, -0.24804688,\n",
       "        0.05371094, -0.0859375 ,  0.17089844, -0.39453125, -0.00156403,\n",
       "       -0.07324219, -0.04614258, -0.16210938, -0.15722656,  0.21289062,\n",
       "       -0.15820312,  0.04394531,  0.28515625,  0.01196289, -0.26953125,\n",
       "       -0.04370117,  0.37109375,  0.04663086, -0.19726562,  0.3046875 ,\n",
       "       -0.36523438, -0.23632812,  0.08056641, -0.04248047, -0.14648438,\n",
       "       -0.06225586, -0.0534668 , -0.05664062,  0.18945312,  0.37109375,\n",
       "       -0.22070312,  0.04638672,  0.02612305, -0.11474609,  0.265625  ,\n",
       "       -0.02453613,  0.11083984, -0.02514648, -0.12060547,  0.05297852,\n",
       "        0.07128906,  0.00063705, -0.36523438, -0.13769531, -0.12890625],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GoogleModel['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Similar words\n",
    "This is one of the interesting features of Word2Vec. You can pass a word and find out the most similar words related to the given word.\n",
    "\n",
    "In the below example, you can see the most relatable word to “king” is “kings” and “queen”. This was possible because of the context learned by the Word2Vec model. Since words like “queen” and “prince” are used in the context of “king”. the numeric word vectors for these words will have similar numbers, hence, the cosine similarity score is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138045430183411),\n",
       " ('queen', 0.6510956883430481),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204220056533813),\n",
       " ('prince', 0.6159993410110474)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding similar words\n",
    "# The most_similar() function finds the cosine similarity of the given word with \n",
    "# other words using the word2Vec representations of each word\n",
    "GoogleModel.most_similar('king', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if a word is present in the Model Vocabulary\n",
    "'Hello' in GoogleModel.key_to_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ab', 'abandon', 'abandoned', 'abc', 'abeam', 'abilities', 'ability',\n",
       "       'able', 'abnormal', 'abnormally',\n",
       "       ...\n",
       "       'zeus', 'zig', 'zip', 'zipped', 'zipper', 'zipping', 'zone', 'zones',\n",
       "       'zoom', 'zooming'],\n",
       "      dtype='object', length=9099)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the list of words which are present in the Document term matrix\n",
    "WordsVocab=CountVectorizedData.columns[:-1]\n",
    " \n",
    "# Printing sample words\n",
    "WordsVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19796"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizedData.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ab           False\n",
       "abandon      False\n",
       "abandoned    False\n",
       "abc          False\n",
       "abeam        False\n",
       "             ...  \n",
       "zipping      False\n",
       "zone         False\n",
       "zones        False\n",
       "zoom         False\n",
       "zooming      False\n",
       "Name: 1, Length: 9099, dtype: bool"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizedData.iloc[1,:-1]==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Converting every sentence to a numeric vector\n",
    "For each word in a sentence, we extract the numeric form of the word and then simply add all the numeric forms for that sentence to represent the sentence.\n",
    "\n",
    "\n",
    "Keypoint of this step :\n",
    "- In order to do transfer learning from pretrained googlenews vector, we need to make sure that in each row of input data, each text para including many words, and each word should has the same dimensions as word in googlenews data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunctionText2Vec(inpTextData):\n",
    "    # Converting the text to numeric data\n",
    "    X = vectorizer.transform(inpTextData)\n",
    "    CountVecData=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    # Creating empty dataframe to hold sentences\n",
    "    W2Vec_Data=pd.DataFrame()\n",
    "    \n",
    "    # Looping through each row for the data\n",
    "    for i in range(CountVecData.shape[0]):\n",
    "\n",
    "        # initiating a sentence with all zeros\n",
    "        Sentence = np.zeros(300)\n",
    "\n",
    "        # Looping thru each word in the sentence and if its present in \n",
    "        # the Word2Vec model then storing its vector. In each row, if the value of the word == 1, and the word is in the list of googledata,\n",
    "        # we add the corresponding vector of googledata to the sentence.\n",
    "        for word in WordsVocab[CountVecData.iloc[i,:]==1]:\n",
    "            #print(word)\n",
    "            if word in GoogleModel.key_to_index.keys():    \n",
    "                Sentence=Sentence+GoogleModel[word]\n",
    "        # Appending the sentence to the dataframe\n",
    "        W2Vec_Data=W2Vec_Data.append(pd.DataFrame([Sentence]))\n",
    "    return(W2Vec_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['blank', 'fallowing', 'help', 'hi', 'link', 'proceed', 'thanks'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordsVocab[CountVectorizedData.iloc[1,:-1]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19796, 300)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since there are so many words... This will take some time :( \n",
    "# Calling the function to convert all the text data to Word2Vec Vectors\n",
    "W2Vec_Data=FunctionText2Vec(TicketData['body'])\n",
    " \n",
    "# Checking the new representation for sentences\n",
    "W2Vec_Data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will have the new data with the format of googlenews data(300 dimensions). Each row corresponding to input sentence which as the same dimensions as googlenews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.075928</td>\n",
       "      <td>1.106567</td>\n",
       "      <td>0.354614</td>\n",
       "      <td>1.532471</td>\n",
       "      <td>-1.502838</td>\n",
       "      <td>0.140747</td>\n",
       "      <td>0.017517</td>\n",
       "      <td>-0.881653</td>\n",
       "      <td>0.570572</td>\n",
       "      <td>0.665894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.742798</td>\n",
       "      <td>0.620361</td>\n",
       "      <td>-1.940552</td>\n",
       "      <td>-0.444405</td>\n",
       "      <td>-0.624146</td>\n",
       "      <td>-0.384447</td>\n",
       "      <td>-0.244141</td>\n",
       "      <td>0.167297</td>\n",
       "      <td>-0.367004</td>\n",
       "      <td>0.325195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.280273</td>\n",
       "      <td>0.475830</td>\n",
       "      <td>0.381348</td>\n",
       "      <td>0.879639</td>\n",
       "      <td>-0.479980</td>\n",
       "      <td>0.546631</td>\n",
       "      <td>0.263428</td>\n",
       "      <td>-0.544907</td>\n",
       "      <td>0.552734</td>\n",
       "      <td>-0.041504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081787</td>\n",
       "      <td>0.452515</td>\n",
       "      <td>-0.370605</td>\n",
       "      <td>-0.068604</td>\n",
       "      <td>0.235535</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>-1.174438</td>\n",
       "      <td>-0.349976</td>\n",
       "      <td>-0.058105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.452057</td>\n",
       "      <td>1.231232</td>\n",
       "      <td>0.295273</td>\n",
       "      <td>3.376648</td>\n",
       "      <td>-3.214890</td>\n",
       "      <td>0.875298</td>\n",
       "      <td>2.255186</td>\n",
       "      <td>-2.470089</td>\n",
       "      <td>4.445435</td>\n",
       "      <td>2.893066</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.701984</td>\n",
       "      <td>2.637276</td>\n",
       "      <td>-3.913803</td>\n",
       "      <td>3.059692</td>\n",
       "      <td>1.958294</td>\n",
       "      <td>1.515013</td>\n",
       "      <td>-0.706757</td>\n",
       "      <td>-3.479492</td>\n",
       "      <td>-0.064575</td>\n",
       "      <td>-0.663330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.141449</td>\n",
       "      <td>-0.917542</td>\n",
       "      <td>0.512207</td>\n",
       "      <td>-0.113647</td>\n",
       "      <td>-1.840729</td>\n",
       "      <td>0.814440</td>\n",
       "      <td>0.889099</td>\n",
       "      <td>-2.341507</td>\n",
       "      <td>2.416199</td>\n",
       "      <td>1.170380</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.317871</td>\n",
       "      <td>0.084351</td>\n",
       "      <td>-0.150513</td>\n",
       "      <td>-0.557800</td>\n",
       "      <td>1.031616</td>\n",
       "      <td>0.032349</td>\n",
       "      <td>0.262897</td>\n",
       "      <td>-1.099567</td>\n",
       "      <td>-0.717957</td>\n",
       "      <td>-0.748886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.316406</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>0.217377</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>-1.221558</td>\n",
       "      <td>-0.266418</td>\n",
       "      <td>0.214355</td>\n",
       "      <td>-0.885132</td>\n",
       "      <td>0.201050</td>\n",
       "      <td>1.045166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103760</td>\n",
       "      <td>0.302124</td>\n",
       "      <td>-0.651001</td>\n",
       "      <td>-0.768311</td>\n",
       "      <td>-0.259766</td>\n",
       "      <td>-0.835436</td>\n",
       "      <td>0.619045</td>\n",
       "      <td>-0.311401</td>\n",
       "      <td>0.296021</td>\n",
       "      <td>0.524902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.628662</td>\n",
       "      <td>0.776978</td>\n",
       "      <td>0.064972</td>\n",
       "      <td>0.415619</td>\n",
       "      <td>-0.816650</td>\n",
       "      <td>-0.481995</td>\n",
       "      <td>0.177505</td>\n",
       "      <td>-0.698730</td>\n",
       "      <td>-0.326172</td>\n",
       "      <td>0.346680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305237</td>\n",
       "      <td>-0.013367</td>\n",
       "      <td>-0.716064</td>\n",
       "      <td>-0.203278</td>\n",
       "      <td>-1.116943</td>\n",
       "      <td>-0.833771</td>\n",
       "      <td>-0.146824</td>\n",
       "      <td>0.082520</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>-0.555786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.573730</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.639832</td>\n",
       "      <td>0.886353</td>\n",
       "      <td>-0.702148</td>\n",
       "      <td>0.094543</td>\n",
       "      <td>1.668457</td>\n",
       "      <td>0.191433</td>\n",
       "      <td>0.765869</td>\n",
       "      <td>0.104004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316772</td>\n",
       "      <td>0.532104</td>\n",
       "      <td>-0.560730</td>\n",
       "      <td>0.672363</td>\n",
       "      <td>-0.108154</td>\n",
       "      <td>-0.241333</td>\n",
       "      <td>-0.457642</td>\n",
       "      <td>-1.194229</td>\n",
       "      <td>-0.069580</td>\n",
       "      <td>0.261475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029175</td>\n",
       "      <td>-0.034668</td>\n",
       "      <td>0.047607</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>-0.271484</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>-0.101074</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.216797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>-0.116211</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.023804</td>\n",
       "      <td>0.134766</td>\n",
       "      <td>-0.257812</td>\n",
       "      <td>-0.457031</td>\n",
       "      <td>0.186523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.133095</td>\n",
       "      <td>-0.460861</td>\n",
       "      <td>-0.677643</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>-0.821899</td>\n",
       "      <td>-0.558777</td>\n",
       "      <td>0.155151</td>\n",
       "      <td>-1.224304</td>\n",
       "      <td>1.358521</td>\n",
       "      <td>1.379639</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.085815</td>\n",
       "      <td>0.912354</td>\n",
       "      <td>-0.701660</td>\n",
       "      <td>-0.823107</td>\n",
       "      <td>0.926270</td>\n",
       "      <td>-0.622314</td>\n",
       "      <td>0.042080</td>\n",
       "      <td>-0.241699</td>\n",
       "      <td>-0.385132</td>\n",
       "      <td>0.326294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.321045</td>\n",
       "      <td>0.710083</td>\n",
       "      <td>0.674194</td>\n",
       "      <td>0.980835</td>\n",
       "      <td>-0.752441</td>\n",
       "      <td>0.708237</td>\n",
       "      <td>0.157959</td>\n",
       "      <td>-0.499390</td>\n",
       "      <td>1.318420</td>\n",
       "      <td>0.585266</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.035553</td>\n",
       "      <td>-0.384155</td>\n",
       "      <td>-0.431274</td>\n",
       "      <td>-0.068848</td>\n",
       "      <td>-0.030029</td>\n",
       "      <td>-1.296814</td>\n",
       "      <td>-0.265381</td>\n",
       "      <td>-0.795868</td>\n",
       "      <td>-0.809082</td>\n",
       "      <td>0.153564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19796 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.075928  1.106567  0.354614  1.532471 -1.502838  0.140747  0.017517   \n",
       "0  -0.280273  0.475830  0.381348  0.879639 -0.479980  0.546631  0.263428   \n",
       "0   1.452057  1.231232  0.295273  3.376648 -3.214890  0.875298  2.255186   \n",
       "0  -0.141449 -0.917542  0.512207 -0.113647 -1.840729  0.814440  0.889099   \n",
       "0  -0.316406  0.035034  0.217377  0.898438 -1.221558 -0.266418  0.214355   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "0  -0.628662  0.776978  0.064972  0.415619 -0.816650 -0.481995  0.177505   \n",
       "0   0.573730  0.010742  0.639832  0.886353 -0.702148  0.094543  1.668457   \n",
       "0   0.029175 -0.034668  0.047607 -0.107910 -0.271484 -0.036621 -0.101074   \n",
       "0   0.133095 -0.460861 -0.677643  0.406250 -0.821899 -0.558777  0.155151   \n",
       "0   0.321045  0.710083  0.674194  0.980835 -0.752441  0.708237  0.157959   \n",
       "\n",
       "         7         8         9    ...       290       291       292       293  \\\n",
       "0  -0.881653  0.570572  0.665894  ... -0.742798  0.620361 -1.940552 -0.444405   \n",
       "0  -0.544907  0.552734 -0.041504  ... -0.081787  0.452515 -0.370605 -0.068604   \n",
       "0  -2.470089  4.445435  2.893066  ... -2.701984  2.637276 -3.913803  3.059692   \n",
       "0  -2.341507  2.416199  1.170380  ... -2.317871  0.084351 -0.150513 -0.557800   \n",
       "0  -0.885132  0.201050  1.045166  ...  0.103760  0.302124 -0.651001 -0.768311   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "0  -0.698730 -0.326172  0.346680  ...  0.305237 -0.013367 -0.716064 -0.203278   \n",
       "0   0.191433  0.765869  0.104004  ...  0.316772  0.532104 -0.560730  0.672363   \n",
       "0  -0.094238  0.132812  0.216797  ...  0.082031  0.173828  0.145508 -0.116211   \n",
       "0  -1.224304  1.358521  1.379639  ... -1.085815  0.912354 -0.701660 -0.823107   \n",
       "0  -0.499390  1.318420  0.585266  ... -1.035553 -0.384155 -0.431274 -0.068848   \n",
       "\n",
       "         294       295       296       297       298       299  \n",
       "0  -0.624146 -0.384447 -0.244141  0.167297 -0.367004  0.325195  \n",
       "0   0.235535  0.026855 -0.306396 -1.174438 -0.349976 -0.058105  \n",
       "0   1.958294  1.515013 -0.706757 -3.479492 -0.064575 -0.663330  \n",
       "0   1.031616  0.032349  0.262897 -1.099567 -0.717957 -0.748886  \n",
       "0  -0.259766 -0.835436  0.619045 -0.311401  0.296021  0.524902  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "0  -1.116943 -0.833771 -0.146824  0.082520  0.082031 -0.555786  \n",
       "0  -0.108154 -0.241333 -0.457642 -1.194229 -0.069580  0.261475  \n",
       "0   0.064453 -0.023804  0.134766 -0.257812 -0.457031  0.186523  \n",
       "0   0.926270 -0.622314  0.042080 -0.241699 -0.385132  0.326294  \n",
       "0  -0.030029 -1.296814 -0.265381 -0.795868 -0.809082  0.153564  \n",
       "\n",
       "[19796 rows x 300 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2Vec_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.075928</td>\n",
       "      <td>1.106567</td>\n",
       "      <td>0.354614</td>\n",
       "      <td>1.532471</td>\n",
       "      <td>-1.502838</td>\n",
       "      <td>0.140747</td>\n",
       "      <td>0.017517</td>\n",
       "      <td>-0.881653</td>\n",
       "      <td>0.570572</td>\n",
       "      <td>0.665894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620361</td>\n",
       "      <td>-1.940552</td>\n",
       "      <td>-0.444405</td>\n",
       "      <td>-0.624146</td>\n",
       "      <td>-0.384447</td>\n",
       "      <td>-0.244141</td>\n",
       "      <td>0.167297</td>\n",
       "      <td>-0.367004</td>\n",
       "      <td>0.325195</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.280273</td>\n",
       "      <td>0.475830</td>\n",
       "      <td>0.381348</td>\n",
       "      <td>0.879639</td>\n",
       "      <td>-0.479980</td>\n",
       "      <td>0.546631</td>\n",
       "      <td>0.263428</td>\n",
       "      <td>-0.544907</td>\n",
       "      <td>0.552734</td>\n",
       "      <td>-0.041504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452515</td>\n",
       "      <td>-0.370605</td>\n",
       "      <td>-0.068604</td>\n",
       "      <td>0.235535</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>-0.306396</td>\n",
       "      <td>-1.174438</td>\n",
       "      <td>-0.349976</td>\n",
       "      <td>-0.058105</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.452057</td>\n",
       "      <td>1.231232</td>\n",
       "      <td>0.295273</td>\n",
       "      <td>3.376648</td>\n",
       "      <td>-3.214890</td>\n",
       "      <td>0.875298</td>\n",
       "      <td>2.255186</td>\n",
       "      <td>-2.470089</td>\n",
       "      <td>4.445435</td>\n",
       "      <td>2.893066</td>\n",
       "      <td>...</td>\n",
       "      <td>2.637276</td>\n",
       "      <td>-3.913803</td>\n",
       "      <td>3.059692</td>\n",
       "      <td>1.958294</td>\n",
       "      <td>1.515013</td>\n",
       "      <td>-0.706757</td>\n",
       "      <td>-3.479492</td>\n",
       "      <td>-0.064575</td>\n",
       "      <td>-0.663330</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.141449</td>\n",
       "      <td>-0.917542</td>\n",
       "      <td>0.512207</td>\n",
       "      <td>-0.113647</td>\n",
       "      <td>-1.840729</td>\n",
       "      <td>0.814440</td>\n",
       "      <td>0.889099</td>\n",
       "      <td>-2.341507</td>\n",
       "      <td>2.416199</td>\n",
       "      <td>1.170380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084351</td>\n",
       "      <td>-0.150513</td>\n",
       "      <td>-0.557800</td>\n",
       "      <td>1.031616</td>\n",
       "      <td>0.032349</td>\n",
       "      <td>0.262897</td>\n",
       "      <td>-1.099567</td>\n",
       "      <td>-0.717957</td>\n",
       "      <td>-0.748886</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.316406</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>0.217377</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>-1.221558</td>\n",
       "      <td>-0.266418</td>\n",
       "      <td>0.214355</td>\n",
       "      <td>-0.885132</td>\n",
       "      <td>0.201050</td>\n",
       "      <td>1.045166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302124</td>\n",
       "      <td>-0.651001</td>\n",
       "      <td>-0.768311</td>\n",
       "      <td>-0.259766</td>\n",
       "      <td>-0.835436</td>\n",
       "      <td>0.619045</td>\n",
       "      <td>-0.311401</td>\n",
       "      <td>0.296021</td>\n",
       "      <td>0.524902</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.075928  1.106567  0.354614  1.532471 -1.502838  0.140747  0.017517   \n",
       "1 -0.280273  0.475830  0.381348  0.879639 -0.479980  0.546631  0.263428   \n",
       "2  1.452057  1.231232  0.295273  3.376648 -3.214890  0.875298  2.255186   \n",
       "3 -0.141449 -0.917542  0.512207 -0.113647 -1.840729  0.814440  0.889099   \n",
       "4 -0.316406  0.035034  0.217377  0.898438 -1.221558 -0.266418  0.214355   \n",
       "\n",
       "          7         8         9  ...       291       292       293       294  \\\n",
       "0 -0.881653  0.570572  0.665894  ...  0.620361 -1.940552 -0.444405 -0.624146   \n",
       "1 -0.544907  0.552734 -0.041504  ...  0.452515 -0.370605 -0.068604  0.235535   \n",
       "2 -2.470089  4.445435  2.893066  ...  2.637276 -3.913803  3.059692  1.958294   \n",
       "3 -2.341507  2.416199  1.170380  ...  0.084351 -0.150513 -0.557800  1.031616   \n",
       "4 -0.885132  0.201050  1.045166  ...  0.302124 -0.651001 -0.768311 -0.259766   \n",
       "\n",
       "        295       296       297       298       299  Priority  \n",
       "0 -0.384447 -0.244141  0.167297 -0.367004  0.325195        P1  \n",
       "1  0.026855 -0.306396 -1.174438 -0.349976 -0.058105        P2  \n",
       "2  1.515013 -0.706757 -3.479492 -0.064575 -0.663330        P1  \n",
       "3  0.032349  0.262897 -1.099567 -0.717957 -0.748886        P2  \n",
       "4 -0.835436  0.619045 -0.311401  0.296021  0.524902        P2  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding the target variable\n",
    "W2Vec_Data.reset_index(inplace=True, drop=True)\n",
    "W2Vec_Data['Priority']=CountVectorizedData['Priority']\n",
    " \n",
    "# Assigning to DataForML variable\n",
    "DataForML=W2Vec_Data\n",
    "DataForML.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13857, 300)\n",
      "(13857,)\n",
      "(5939, 300)\n",
      "(5939,)\n"
     ]
    }
   ],
   "source": [
    "# Separate Target Variable and Predictor Variables\n",
    "TargetVariable=DataForML.columns[-1]\n",
    "Predictors=DataForML.columns[:-1]\n",
    " \n",
    "X=DataForML[Predictors].values\n",
    "y=DataForML[TargetVariable].values\n",
    " \n",
    "# Split the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=428)\n",
    " \n",
    "# Sanity check for the sampled data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Standardization/Normalization\n",
    "This is an optional step. It can speed up the processing of the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13857, 300)\n",
      "(13857,)\n",
      "(5939, 300)\n",
      "(5939,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Choose either standardization or Normalization\n",
    "# On this data Min Max Normalization is used because we need to fit Naive Bayes\n",
    " \n",
    "# Choose between standardization and MinMAx normalization\n",
    "#PredictorScaler=StandardScaler()\n",
    "PredictorScaler=MinMaxScaler()\n",
    " \n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(X)\n",
    " \n",
    "# Generating the standardized values of X\n",
    "X=PredictorScalerFit.transform(X)\n",
    " \n",
    "# Split the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=428)\n",
    " \n",
    "# Sanity check for the sampled data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ML classification models\n",
    "Now the data is ready for machine learning. There are 300-predictors and one target variable. We will use the below algorithms and select the best one out of them based on the accuracy scores you can add more algorithms to this list as per your preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "This is a distance-based supervised ML algorithm. Make sure you standardize/normalize the data before using this algorithm, otherwise the accuracy will be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=15)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          P1       0.57      0.70      0.63      2006\n",
      "          P2       0.49      0.25      0.34      1653\n",
      "          P3       0.75      0.87      0.81      2280\n",
      "\n",
      "    accuracy                           0.64      5939\n",
      "   macro avg       0.61      0.61      0.59      5939\n",
      "weighted avg       0.62      0.64      0.61      5939\n",
      "\n",
      "[[1404  334  268]\n",
      " [ 860  420  373]\n",
      " [ 209   98 1973]]\n",
      "Accuracy of the model on Testing Sample Data: 0.61\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbor(KNN)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=15)\n",
    " \n",
    "# Printing all the parameters of KNN\n",
    "print(clf)\n",
    " \n",
    "# Creating the model on Training Data\n",
    "KNN=clf.fit(X_train,y_train)\n",
    "prediction=KNN.predict(X_test)\n",
    " \n",
    "# Measuring accuracy on Testing Data\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(metrics.confusion_matrix(y_test, prediction))\n",
    " \n",
    "# Printing the Overall Accuracy of the model\n",
    "F1_Score=metrics.f1_score(y_test, prediction, average='weighted')\n",
    "print('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n",
    " \n",
    "# Importing cross validation function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# Running 10-Fold Cross validation on a given algorithm\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "#Accuracy_Values=cross_val_score(KNN, X , y, cv=10, scoring='f1_weighted')\n",
    "#print('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\n",
    "#print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))\n",
    " \n",
    "# Plotting the feature importance for Top 10 most important columns\n",
    "# There is no built-in method to get feature importance in KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost\n",
    "This is a tree based boosting algorithm. If the data is not high dimensional, we can use this algorithm. otherwise it takes lot of time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),\n",
      "                   learning_rate=0.01, n_estimators=20)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          P1       0.46      0.76      0.57      2006\n",
      "          P2       0.31      0.15      0.20      1653\n",
      "          P3       0.70      0.56      0.62      2280\n",
      "\n",
      "    accuracy                           0.51      5939\n",
      "   macro avg       0.49      0.49      0.46      5939\n",
      "weighted avg       0.51      0.51      0.49      5939\n",
      "\n",
      "[[1516  234  256]\n",
      " [1124  246  283]\n",
      " [ 688  312 1280]]\n",
      "Accuracy of the model on Testing Sample Data: 0.49\n"
     ]
    }
   ],
   "source": [
    "# Adaboost \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    " \n",
    "# Choosing Decision Tree with 1 level as the weak learner\n",
    "DTC=DecisionTreeClassifier(max_depth=2)\n",
    "clf = AdaBoostClassifier(n_estimators=20, base_estimator=DTC ,learning_rate=0.01)\n",
    " \n",
    "# Printing all the parameters of Adaboost\n",
    "print(clf)\n",
    " \n",
    "# Creating the model on Training Data\n",
    "AB=clf.fit(X_train,y_train)\n",
    "prediction=AB.predict(X_test)\n",
    " \n",
    "# Measuring accuracy on Testing Data\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(metrics.confusion_matrix(y_test, prediction))\n",
    " \n",
    "# Printing the Overall Accuracy of the model\n",
    "F1_Score=metrics.f1_score(y_test, prediction, average='weighted')\n",
    "print('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n",
    " \n",
    "# Importing cross validation function from sklearn\n",
    "#from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# Running 10-Fold Cross validation on a given algorithm\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "#Accuracy_Values=cross_val_score(AB, X , y, cv=10, scoring='f1_weighted')\n",
    "#print('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\n",
    "#print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))\n",
    " \n",
    "# Plotting the feature importance for Top 10 most important columns\n",
    "#%matplotlib inline\n",
    "#feature_importances = pd.Series(AB.feature_importances_, index=Predictors)\n",
    "#feature_importances.nlargest(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55b5f86fc7a8e13cda17962ae7c8afc78c81afeb23ec7a229dffbbed448f19ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
